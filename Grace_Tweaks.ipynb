{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIvzzZtzvS7a"
      },
      "outputs": [],
      "source": [
        "# Introduction to Neural Networks (CSE 40868/60868), Spring 2023\n",
        "# University of Notre Dame\n",
        "# Final Project\n",
        "# _______________________________________________________________________________\n",
        "# Patrick Hsiao and Grace Bezold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install retina-face\n",
        "from retinaface import RetinaFace\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.functional import resize\n",
        "from torchvision.transforms import Resize, ToTensor\n",
        "from tqdm import tqdm\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.datasets as datasets"
      ],
      "metadata": {
        "id": "p5nE42HPv2hb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69473757-1498-4d7f-d27b-bed99a87a625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting retina-face\n",
            "  Downloading retina_face-0.0.13-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from retina-face) (8.4.0)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from retina-face) (2.12.0)\n",
            "Requirement already satisfied: opencv-python>=3.4.4 in /usr/local/lib/python3.10/dist-packages (from retina-face) (4.7.0.72)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from retina-face) (4.6.6)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from retina-face) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->retina-face) (4.65.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->retina-face) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->retina-face) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->retina-face) (3.12.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->retina-face) (4.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (1.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (2.12.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (0.32.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (2.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (1.14.1)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (2.12.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (0.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (3.20.3)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (0.4.8)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (16.0.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (2.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (1.54.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (67.7.2)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (23.3.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (4.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->retina-face) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=1.9.0->retina-face) (1.10.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=1.9.0->retina-face) (0.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->retina-face) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->retina-face) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->retina-face) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->retina-face) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->retina-face) (0.7.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->retina-face) (2.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=3.10.1->retina-face) (2.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (1.7.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->retina-face) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->retina-face) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->retina-face) (0.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->retina-face) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->retina-face) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->retina-face) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->retina-face) (3.2.2)\n",
            "Installing collected packages: retina-face\n",
            "Successfully installed retina-face-0.0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAbhLBw8xBb5",
        "outputId": "4eb8b1f8-958f-4539-92b2-1b8d3a111035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vt3CX03ptlE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set image directory"
      ],
      "metadata": {
        "id": "yPQIXWAjtcIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get images from Google Drive\n",
        "image_dir = '/content/drive/MyDrive/face_rec_data'"
      ],
      "metadata": {
        "id": "EEZYF6IttbgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure Images to fit only faces\n",
        "*DO NOT RUN THIS CELL*\n",
        "we have already uploaded the cropped images in face_rec_data so this does not need to be run"
      ],
      "metadata": {
        "id": "eV8UR2NCtXjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through all images in the directory\n",
        "for folder_path in os.listdir(image_dir):\n",
        "    folders = os.path.join(image_dir, folder_path)\n",
        "\n",
        "    for file_name in os.listdir(folders):\n",
        "\n",
        "\n",
        "      # Check if the file is an image\n",
        "      if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "\n",
        "        # Get the full path of the image file\n",
        "        image_path = os.path.join(folders, file_name)\n",
        "\n",
        "        # Read the image\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Detect faces in the image\n",
        "        faces = RetinaFace.detect_faces(image)\n",
        "        print(faces)\n",
        "\n",
        "        try:\n",
        "        # Extract the bounding box of the face\n",
        "            x1, y1, x2, y2 = faces['face_1']['facial_area']\n",
        "\n",
        "        # Crop the face from the image\n",
        "            cropped_image = image[y1:y2, x1:x2]\n",
        "\n",
        "        # Convert to PIL Image\n",
        "            pil_image = Image.fromarray(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "\n",
        "        # Save the cropped image with a new file name\n",
        "            cropped_path = os.path.join(folders, file_name)\n",
        "            pil_image.save(cropped_path)\n",
        "        except:\n",
        "            print(\"No faces were detected in the image\")\n",
        "            continue\n"
      ],
      "metadata": {
        "id": "awDFIoHyxjj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory containing the images and the transformation to apply to each image\n",
        "transform = transforms.Compose([\n",
        "    Resize((256, 256)),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "# Create the ImageFolder dataset\n",
        "image_dataset = ImageFolder(image_dir, transform=transform)\n",
        "print(image_dataset)\n",
        "\n",
        "# Split the dataset into training, validation, and testing sets\n",
        "train_size = int(len(image_dataset) * 0.8)\n",
        "val_size = int(len(image_dataset) * 0.1)\n",
        "test_size = len(image_dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(image_dataset, [train_size, val_size, test_size])\n",
        "classes = ['grace','other','patrick']\n",
        "\n",
        "# Create data loaders for the training, validation, and testing sets\n",
        "batch_size = 32\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Print the shapes of the training, validation, and testing sets\n",
        "data, labels = next(iter(train_loader))\n",
        "print(\"Training set:\", data.shape, labels.shape)\n",
        "data, labels = next(iter(val_loader))\n",
        "print(\"Validation set:\", data.shape, labels.shape)\n",
        "data, labels = next(iter(test_loader))\n",
        "print(\"Testing set:\", data.shape, labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5C2L3Xh_gBQ",
        "outputId": "c1e1394c-17cd-40b9-edc8-73a501e4259a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ImageFolder\n",
            "    Number of datapoints: 311\n",
            "    Root location: /content/drive/MyDrive/face_rec_data\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=warn)\n",
            "               ToTensor()\n",
            "           )\n",
            "Training set: torch.Size([32, 3, 256, 256]) torch.Size([32])\n",
            "Validation set: torch.Size([31, 3, 256, 256]) torch.Size([31])\n",
            "Testing set: torch.Size([32, 3, 256, 256]) torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Network"
      ],
      "metadata": {
        "id": "xlvKZx4G4FL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self, numChannels, numClasses):\n",
        "        super(CNN, self).__init__()\n",
        "        self.classes = numClasses\n",
        "\n",
        "\n",
        "        # Convolutional layers:\n",
        "        self.conv1 = nn.Conv2d(in_channels = numChannels, out_channels=96,kernel_size=(11,11),stride=(4,4))\n",
        "        self.conv2 = nn.Conv2d(in_channels = 96, out_channels = 256, kernel_size = (5,5), stride=(1,1))\n",
        "        self.conv3 = nn.Conv2d(in_channels = 256, out_channels = 384, kernel_size = (3,3),stride=(1,1))\n",
        "        self.conv4 = nn.Conv2d(in_channels = 384, out_channels = 256, kernel_size = (6,6), stride = (1,1))\n",
        "        #self.conv5 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = (3,3), stride = (1,1))\n",
        "\n",
        "        # Activation function:\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Pooling layer:\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size = (2,2), stride = (2,2))\n",
        "\n",
        "        # Batch normalization layers:\n",
        "        self.batchnorm1 = nn.BatchNorm2d(num_features = 96)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(num_features = 256)\n",
        "\n",
        "        # Fully-connected layers:\n",
        "        self.fc1 = nn.Linear(in_features = 2304,out_features = 1024)\n",
        "        self.fc2 = nn.Linear(in_features = 1024, out_features = 3)\n",
        "\n",
        "        #add drop out\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "    # Evaluation function\n",
        "    def evaluate(self, model, dataloader, classes, device):\n",
        "\n",
        "        # We need to switch the model into the evaluation mode\n",
        "        model.eval()\n",
        "        \n",
        "        # Prepare to count predictions for each class\n",
        "        correct_pred = {classname: 0 for classname in classes}\n",
        "        total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "        # For all test data samples:\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "            images = images.detach().cpu().numpy()\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            predictions = predictions.detach().cpu().numpy()\n",
        "\n",
        "            # Count the correct predictions for each class\n",
        "            for label, prediction in zip(labels, predictions):\n",
        "                \n",
        "                # If you want to see real and predicted labels for all samples:\n",
        "                # print(\"Real class: \" + classes[label] + \", predicted = \" + classes[prediction])\n",
        "                \n",
        "                if label == prediction:\n",
        "                    correct_pred[classes[label]] += 1\n",
        "                total_pred[classes[label]] += 1\n",
        "\n",
        "        # Calculate the overall accuracy on the test set\n",
        "        acc = sum(correct_pred.values()) / sum(total_pred.values())\n",
        "\n",
        "        return acc\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = resize(x, size=[256])\n",
        "\n",
        "\n",
        "        # Convolutional, ReLU, MacPooling and Batchnorm layers go first\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.relu(x)\n",
        "        #x = self.conv5(x)\n",
        "        #x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "\n",
        "\n",
        "        # fully-connected layer, we need to \"flatten\" our tensors\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # Finally, we need our two-layer perceptron (two fully-connected layers) at the end of the network:\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "_JPVnqHQ4FAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5"
      ],
      "metadata": {
        "id": "QqLlIH4P6942"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Specify the operation mode:\n",
        "    # 'train' = training with your train and validation data splits\n",
        "    # 'eval'  = evaluation of the trained model with your test data split \n",
        "    mode = 'eval'\n",
        "\n",
        "    # Path where you plan to save the best model during training\n",
        "    my_best_model = \"/content/drive/MyDrive/face_rec_data/face_recog_best_model.pth\"\n",
        "    print(my_best_model)\n",
        "\n",
        "\n",
        "\n",
        "    # Set the device (GPU or CPU, depending on availability)\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"Currently using device: \", device)\n",
        "\n",
        "    # Initialize the model and print out its configuration\n",
        "    model = CNN(numChannels = 3, numClasses = 3)\n",
        "    model.to(device)\n",
        "\n",
        "    print(\"\\n\\nModel summary:\\n\\n\")\n",
        "    summary(model, input_size=(3, 32, 32))\n",
        "\n",
        "    if mode == \"train\":\n",
        "\n",
        "        print(\"\\n\\nTraining starts!\\n\\n\")\n",
        "        \n",
        "        model.train()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        \n",
        "        running_loss = .0\n",
        "        best_acc = .0\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Starting epoch {epoch + 1}\")\n",
        "            for idx, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "\n",
        "                # Get the inputs (data is a list of [inputs, labels])\n",
        "                inputs, labels = data\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                loss = loss.detach().cpu().numpy()\n",
        "                inputs = inputs.detach().cpu().numpy()\n",
        "                labels = labels.detach().cpu().numpy()\n",
        "                running_loss += loss\n",
        "\n",
        "            # Evaluate the accuracy after each epoch\n",
        "            acc = model.evaluate(model, val_loader, classes, device)\n",
        "            if acc > best_acc:\n",
        "                print(f\"Better validation accuracy achieved: {acc * 100:.2f}%\")\n",
        "                best_acc = acc\n",
        "                print(f\"Saving this model as: {my_best_model}\")\n",
        "                torch.save(model.state_dict(), my_best_model)\n",
        "\n",
        "    # And here we evaluate the trained model with the test data\n",
        "    elif mode == \"eval\":\n",
        "\n",
        "        print(\"\\n\\nValidating the trained model:\")\n",
        "        print(f\"Loading checkpoint from {my_best_model}\")\n",
        "        model.load_state_dict(torch.load(my_best_model))\n",
        "        acc = model.evaluate(model, test_loader, classes, device)\n",
        "        print(f\"Accuracy on the test (unknown) data: {acc * 100:.2f}%\")\n",
        "\n",
        "    else:\n",
        "        print(\"'mode' argument should either be 'train' or 'eval'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_krdq3xm7HS0",
        "outputId": "236e6706-51e8-4ad7-f60f-6aa7d6cc4075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/face_rec_data/face_recog_best_model.pth\n",
            "Currently using device:  cpu\n",
            "\n",
            "\n",
            "Model summary:\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 96, 62, 62]          34,944\n",
            "              ReLU-2           [-1, 96, 62, 62]               0\n",
            "         MaxPool2d-3           [-1, 96, 31, 31]               0\n",
            "       BatchNorm2d-4           [-1, 96, 31, 31]             192\n",
            "            Conv2d-5          [-1, 256, 27, 27]         614,656\n",
            "              ReLU-6          [-1, 256, 27, 27]               0\n",
            "         MaxPool2d-7          [-1, 256, 13, 13]               0\n",
            "       BatchNorm2d-8          [-1, 256, 13, 13]             512\n",
            "            Conv2d-9          [-1, 384, 11, 11]         885,120\n",
            "             ReLU-10          [-1, 384, 11, 11]               0\n",
            "           Conv2d-11            [-1, 256, 6, 6]       3,539,200\n",
            "             ReLU-12            [-1, 256, 6, 6]               0\n",
            "        MaxPool2d-13            [-1, 256, 3, 3]               0\n",
            "          Dropout-14                 [-1, 2304]               0\n",
            "           Linear-15                 [-1, 1024]       2,360,320\n",
            "             ReLU-16                 [-1, 1024]               0\n",
            "           Linear-17                    [-1, 3]           3,075\n",
            "================================================================\n",
            "Total params: 7,438,019\n",
            "Trainable params: 7,438,019\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 11.45\n",
            "Params size (MB): 28.37\n",
            "Estimated Total Size (MB): 39.83\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\n",
            "Validating the trained model:\n",
            "Loading checkpoint from /content/drive/MyDrive/face_rec_data/face_recog_best_model.pth\n",
            "Accuracy on the test (unknown) data: 59.38%\n"
          ]
        }
      ]
    }
  ]
}